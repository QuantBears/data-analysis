{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "accessKeys = pd.read_csv(\"../../quant-bears_accessKeys.csv\")\n",
    "session = boto3.Session(\n",
    "\taws_access_key_id=accessKeys.loc[0, \"Access key ID\"],\n",
    "\taws_secret_access_key=accessKeys.loc[0, \"Secret access key\"]\n",
    ")\n",
    "\n",
    "s3_collection_path = \"s3://quant-bears-data-collection/raw-data/\"\n",
    "s3_price_collection_path = \"s3://quant-bears-data-collection/raw-resolved-price/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seekingAlpha.seekingAlphaBulkMetrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 12%|█▎        | 1/8 [00:01<00:07,  1.09s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 25%|██▌       | 2/8 [00:02<00:07,  1.29s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 38%|███▊      | 3/8 [00:03<00:06,  1.24s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 50%|█████     | 4/8 [00:05<00:05,  1.28s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 62%|██████▎   | 5/8 [00:05<00:03,  1.09s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 75%|███████▌  | 6/8 [00:06<00:02,  1.07s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      " 88%|████████▊ | 7/8 [00:07<00:01,  1.03s/it]/var/folders/tq/51dxx6813y7g8dg44h973dmr0000gn/T/ipykernel_81092/716229386.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
      "100%|██████████| 8/8 [00:09<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gurufocus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12708, 337)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sources = [\"seekingAlpha.seekingAlphaBulkMetrics\", \"gurufocus\"]\n",
    "sources_dict = dict((source, wr.s3.list_objects(s3_collection_path + source + \"/\", boto3_session=session)) for source in data_sources)\n",
    "df_dict = {}\n",
    "for source in data_sources:\n",
    "\tdfs = []\n",
    "\tprint(source)\n",
    "\tfor path in tqdm(sources_dict[source]):\n",
    "\t\tnew_df = wr.s3.read_parquet(path, boto3_session=session)\n",
    "\t\tnew_df[\"date\"] = path.split(\"/\")[-1].split(\".\")[0]\n",
    "\t\tdfs.append(new_df)\n",
    "\n",
    "\tdf_dict[source] = pd.concat(dfs, axis = 0)\n",
    "joined_df = pd.concat([df.set_index([\"date\", \"ticker\"]) for df in df_dict.values()], axis = 1)\n",
    "joined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Drop all rows without \"primary_price\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12700, 337)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1_df = joined_df[~joined_df[\"primary_price\"].isna()]\n",
    "step1_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Drop all columns with >80% NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12700, 294)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_pct_per_col = step1_df.isna().sum() / step1_df.shape[0]\n",
    "nan_cols = step1_df.columns[nan_pct_per_col > .8]\n",
    "step2_df = step1_df.drop(nan_cols, axis = 1)\n",
    "step2_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Convert category columns to one-hot columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12700, 358)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_df = step2_df.select_dtypes([int, float])\n",
    "object_df = step2_df.select_dtypes(\"string\")\n",
    "onehot_df = pd.get_dummies(object_df).astype(\"float\")\n",
    "step3_df = pd.concat([float_df, onehot_df], axis = 1)\n",
    "step3_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Median and Zero Imputation on columns with stddev effect of <10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_median_or_zero_by_thresh(df: pd.DataFrame, thresh: float):\n",
    "\tdef stddev_diff(col):\n",
    "\t\tstddev_median_diff = col.fillna(col.median()).std() / col.std()\n",
    "\t\tstddev_zero_diff = col.fillna(0.).std() / col.std()\n",
    "\t\treturn (stddev_median_diff, stddev_zero_diff, \"median\" if stddev_median_diff <= stddev_zero_diff else \"zero\")\n",
    "\t\n",
    "\tfloat_df = df.select_dtypes([int, float])\n",
    "\tstddev_diffs = float_df.apply(stddev_diff).rename(index = {0: \"median_fill\", 1: \"zero_fill\", 2: \"method\"})\n",
    "\tdist_from_1 = (1. - stddev_diffs.T[[\"median_fill\", \"zero_fill\"]]).abs()\n",
    "\tdist_from_1[\"min_diff\"] = dist_from_1.min(axis = 1)\n",
    "\tpassed_diffs = stddev_diffs.T[dist_from_1[\"min_diff\"] <= thresh]\n",
    "\n",
    "\tdef apply_imputation(col: pd.Series):\n",
    "\t\tif col.name in passed_diffs.index:\n",
    "\t\t\tmethod = passed_diffs.loc[col.name][\"method\"]\n",
    "\t\t\tif method == \"zero\":\n",
    "\t\t\t\treturn col.fillna(0.)\n",
    "\t\t\telif method == \"median\":\n",
    "\t\t\t\treturn col.fillna(col.median())\n",
    "\t\treturn col\n",
    "\t\n",
    "\treturn df.apply(apply_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs before imputation:\n",
      "652309\n",
      "NaNs after imputation:\n",
      "405045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12700, 358)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_df = step3_df.select_dtypes([int, float])\n",
    "object_df = step3_df.select_dtypes(\"object\")\n",
    "\n",
    "print(\"NaNs before imputation:\")\n",
    "print(float_df.isna().sum().sum())\n",
    "\n",
    "imputed_float_df = fill_median_or_zero_by_thresh(float_df, .1)\n",
    "\n",
    "print(\"NaNs after imputation:\")\n",
    "print(imputed_float_df.isna().sum().sum())\n",
    "\n",
    "step4_df = pd.concat([imputed_float_df, object_df], axis = 1)\n",
    "step4_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Predict remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:35<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "step5_df = step4_df.copy()\n",
    "nan_cols = step4_df.columns[step4_df.isna().sum() > 0]\n",
    "X = step4_df.drop(nan_cols, axis = 1)\n",
    "for col in tqdm(nan_cols):\n",
    "\ty_train = step4_df[col].dropna()\n",
    "\tmask = X.index.isin(y_train.index)\n",
    "\tX_train = X[mask]\n",
    "\tX_test = X[~mask]\n",
    "\n",
    "\tlinreg = LinearRegression()\n",
    "\n",
    "\tlinreg.fit(X_train, y_train)\n",
    "\ty_pred = linreg.predict(X_test)\n",
    "\tdescriptions = y_train.describe()\n",
    "\n",
    "\ty_pred = y_pred.clip(min = descriptions[\"min\"], max=descriptions[\"max\"])\n",
    "\tpred_col = pd.Series(y_pred, index = X_test.index).reindex(X.index)\n",
    "\tstep5_df[col] = step4_df[col].where(~step4_df[col].isna(), pred_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No NaNs!\n",
    "step5_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QuantBearsDataAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
